{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92724cfd",
   "metadata": {},
   "source": [
    "# Step 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23009e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = pathlib.Path(os.getcwd()) \n",
    "csvdir_path = notebook_dir.parent / \"data\" / \"external\" \n",
    "file_name = \"FPA_FOD_Plus.csv\"\n",
    "file_path = f\"{csvdir_path}/{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4bd0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/nv1yy17503b71cs9y247p65w0000gn/T/ipykernel_72079/477084150.py:1: DtypeWarning: Columns (9,11,13,14,15,16,17,52,53,60,62,63,64,307) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_init = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "df_init = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a502d",
   "metadata": {},
   "source": [
    "## Getting to know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a018ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the dataset: 308\n",
      "The number of columns in the dataset: 2302521\n",
      "Column names int the dataset: ['FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_PLUS_INCIDENT_JOIN_ID', 'ICS_209_PLUS_COMPLEX_JOIN_ID', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE', 'NWCG_CAUSE_AGE_CATEGORY', 'CONT_DATE', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_DESCR', 'STATE', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'LatLong_State', 'LatLong_County', 'NPL', 'Mang_Type', 'Mang_Name', 'Des_Tp', 'GAP_Sts', 'GAP_Prity', 'EVH', 'EVT', 'EVH_1km', 'EVT_1km', 'EVC', 'EVC_1km', 'NAME', 'MOD_NDVI_12m', 'MOD_EVI_12m', 'Land_Cover', 'Land_Cover_1km', 'rpms', 'rpms_1km', 'Population', 'Popo_1km', 'GACCAbbrev', 'GACC_PL', 'GACC_New fire', 'GACC_New LF', 'GACC_Uncont LF', 'GACC_Type 1 IMTs', 'GACC_Type 2 IMTs', 'GACC_NIMO Teams', 'GACC_Area Command Teams', 'GACC_Fire Use Teams', 'GDP', 'pr_Normal', 'tmmn_Normal', 'tmmx_Normal', 'rmin_Normal', 'rmax_Normal', 'sph_Normal', 'srad_Normal', 'fm100_Normal', 'fm1000_Normal', 'bi_Normal', 'vpd_Normal', 'erc_Normal', 'DF_PFS', 'AF_PFS', 'HDF_PFS', 'DSF_PFS', 'EBF_PFS', 'EALR_PFS', 'EBLR_PFS', 'EPLR_PFS', 'HBF_PFS', 'LLEF_PFS', 'LIF_PFS', 'LMI_PFS', 'MHVF_PFS', 'PM25F_PFS', 'HSEF', 'P100_PFS', 'P200_PFS', 'LPF_PFS', 'NPL_PFS', 'RMP_PFS', 'TSDF_PFS', 'TPF', 'TF_PFS', 'UF_PFS', 'WF_PFS', 'M_WTR', 'M_WKFC', 'M_CLT', 'M_ENY', 'M_TRN', 'M_HSG', 'M_PLN', 'M_HLTH', 'SM_C', 'SM_PFS', 'EPLRLI', 'EALRLI', 'EBLRLI', 'PM25LI', 'EBLI', 'DPMLI', 'TPLI', 'LPMHVLI', 'HBLI', 'RMPLI', 'SFLI', 'HWLI', 'WDLI', 'DLI', 'ALI', 'HDLI', 'LLELI', 'LILHSE', 'PLHSE', 'LMILHSE', 'ULHSE', 'EPL_ET', 'EAL_ET', 'EBL_ET', 'EB_ET', 'PM25_ET', 'DS_ET', 'TP_ET', 'LPP_ET', 'HB_ET', 'RMP_ET', 'NPL_ET', 'TSDF_ET', 'WD_ET', 'DB_ET', 'A_ET', 'HD_ET', 'LLE_ET', 'UN_ET', 'LISO_ET', 'POV_ET', 'LMI_ET', 'IA_LMI_ET', 'IA_UN_ET', 'IA_POV_ET', 'TC', 'CC', 'IAULHSE', 'IAPLHSE', 'IALMILHSE', 'IALMIL_87', 'IAPLHS_88', 'IAULHS_89', 'LHE', 'IALHE', 'IAHSEF', 'CA', 'NCA', 'CA_LT20', 'M_CLT_EOMI', 'M_ENY_EOMI', 'M_TRN_EOMI', 'M_HSG_EOMI', 'M_PLN_EOMI', 'M_WTR_EOMI', 'M_HLTH_102', 'M_WKFC_103', 'FPL200S', 'M_WKFC_105', 'M_EBSI', 'UI_EXP', 'THRHLD', 'No_FireStation_1.0km', 'No_FireStation_5.0km', 'No_FireStation_10.0km', 'No_FireStation_20.0km', 'FRG_1km', 'FRG', 'TRI_1km', 'TRI', 'Aspect_1km', 'Elevation_1km', 'Elevation', 'Slope_1km', 'Aspect', 'Slope', 'GHM', 'TPI', 'TPI_1km', 'TRACT', 'RPL_THEMES', 'RPL_THEME1', 'EPL_POV', 'EPL_UNEMP', 'EPL_PCI', 'EPL_NOHSDP', 'RPL_THEME2', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT', 'RPL_THEME3', 'EPL_MINRTY', 'EPL_LIMENG', 'RPL_THEME4', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD', 'EPL_NOVEH', 'EPL_GROUPQ', 'road_county_dis', 'road_interstate_dis', 'road_common_name_dis', 'road_other_dis', 'road_state_dis', 'road_US_dis', 'Ecoregion_US_L4CODE', 'Ecoregion_US_L3CODE', 'Ecoregion_NA_L3CODE', 'Ecoregion_NA_L2CODE', 'Ecoregion_NA_L1CODE', 'SDI', 'Annual_etr', 'Annual_precipitation', 'Annual_tempreture', 'Aridity_index', 'Evacuation', 'pr', 'tmmn', 'tmmx', 'rmin', 'rmax', 'sph', 'vs', 'th', 'srad', 'etr', 'fm100', 'fm1000', 'bi', 'vpd', 'erc', 'pr_5D_mean', 'tmmn_5D_mean', 'tmmx_5D_mean', 'rmin_5D_mean', 'rmax_5D_mean', 'sph_5D_mean', 'vs_5D_mean', 'th_5D_mean', 'srad_5D_mean', 'etr_5D_mean', 'fm100_5D_mean', 'fm1000_5D_mean', 'bi_5D_mean', 'vpd_5D_mean', 'erc_5D_mean', 'pr_5D_min', 'pr_5D_max', 'tmmn_5D_max', 'tmmx_5D_max', 'rmin_5D_min', 'rmax_5D_min', 'sph_5D_min', 'vs_5D_max', 'th_5D_max', 'srad_5D_max', 'etr_5D_max', 'fm100_5D_min', 'fm1000_5D_min', 'bi_5D_max', 'vpd_5D_max', 'erc_5D_max', 'tmmn_Percentile', 'tmmx_Percentile', 'sph_Percentile', 'vs_Percentile', 'fm100_Percentile', 'bi_Percentile', 'vpd_Percentile', 'erc_Percentile', 'NDVI-1day', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'CheatGrass', 'ExoticAnnualGrass', 'Medusahead', 'PoaSecunda', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# 1. how many columns does the dataset contain?\n",
    "print(f\"The number of columns in the dataset: {df_init.shape[1]}\")\n",
    "\n",
    "# 2. how many rows are there in the dataset?\n",
    "print(f\"The number of columns in the dataset: {df_init.shape[0]}\")\n",
    "\n",
    "# 3. what are the column names?\n",
    "print(f\"Column names int the dataset: {list(df_init.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e23ac04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IAHSEF                           2302521\n",
       "IAULHS_89                        2302521\n",
       "IAPLHS_88                        2302521\n",
       "IALMIL_87                        2302521\n",
       "ICS_209_PLUS_COMPLEX_JOIN_ID     2301454\n",
       "road_interstate_dis              2300797\n",
       "road_US_dis                      2297213\n",
       "COMPLEX_NAME                     2296693\n",
       "road_other_dis                   2290753\n",
       "MTBS_ID                          2288759\n",
       "MTBS_FIRE_NAME                   2288759\n",
       "ICS_209_PLUS_INCIDENT_JOIN_ID    2269099\n",
       "road_state_dis                   2268194\n",
       "road_county_dis                  2261432\n",
       "NWCG_CAUSE_AGE_CATEGORY          2227010\n",
       "No_FireStation_1.0km             2110937\n",
       "Evacuation                       2009434\n",
       "CheatGrass                       1921616\n",
       "ExoticAnnualGrass                1921616\n",
       "Medusahead                       1921616\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. how many missing values are there in each column?\n",
    "df_init.isna().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29839fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals = df_init.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a53161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IAHSEF                           2302521\n",
       "IAULHS_89                        2302521\n",
       "IAPLHS_88                        2302521\n",
       "IALMIL_87                        2302521\n",
       "ICS_209_PLUS_COMPLEX_JOIN_ID     2301454\n",
       "road_interstate_dis              2300797\n",
       "road_US_dis                      2297213\n",
       "COMPLEX_NAME                     2296693\n",
       "road_other_dis                   2290753\n",
       "MTBS_ID                          2288759\n",
       "MTBS_FIRE_NAME                   2288759\n",
       "ICS_209_PLUS_INCIDENT_JOIN_ID    2269099\n",
       "road_state_dis                   2268194\n",
       "road_county_dis                  2261432\n",
       "NWCG_CAUSE_AGE_CATEGORY          2227010\n",
       "No_FireStation_1.0km             2110937\n",
       "Evacuation                       2009434\n",
       "CheatGrass                       1921616\n",
       "ExoticAnnualGrass                1921616\n",
       "Medusahead                       1921616\n",
       "PoaSecunda                       1921616\n",
       "FIRE_CODE                        1905220\n",
       "LOCAL_FIRE_REPORT_ID             1824846\n",
       "geometry                         1730504\n",
       "GACC_Fire Use Teams              1710985\n",
       "GACC_Type 2 IMTs                 1183441\n",
       "GACC_New LF                      1183441\n",
       "GACC_Area Command Teams          1183441\n",
       "GACC_NIMO Teams                  1183441\n",
       "GACCAbbrev                       1183441\n",
       "GACC_PL                          1183441\n",
       "GACC_New fire                    1183441\n",
       "GACC_Type 1 IMTs                 1183441\n",
       "GACC_Uncont LF                   1183441\n",
       "FIRE_NAME                         995395\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_vals.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5eb216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2302521, 308)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56608f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns with missing values greater than 1 million: 34\n",
      "Those columns are:\n",
      "IAHSEF\n",
      "IAULHS_89\n",
      "IAPLHS_88\n",
      "IALMIL_87\n",
      "ICS_209_PLUS_COMPLEX_JOIN_ID\n",
      "road_interstate_dis\n",
      "road_US_dis\n",
      "COMPLEX_NAME\n",
      "road_other_dis\n",
      "MTBS_ID\n",
      "MTBS_FIRE_NAME\n",
      "ICS_209_PLUS_INCIDENT_JOIN_ID\n",
      "road_state_dis\n",
      "road_county_dis\n",
      "NWCG_CAUSE_AGE_CATEGORY\n",
      "No_FireStation_1.0km\n",
      "Evacuation\n",
      "CheatGrass\n",
      "ExoticAnnualGrass\n",
      "Medusahead\n",
      "PoaSecunda\n",
      "FIRE_CODE\n",
      "LOCAL_FIRE_REPORT_ID\n",
      "geometry\n",
      "GACC_Fire Use Teams\n",
      "GACC_Type 2 IMTs\n",
      "GACC_New LF\n",
      "GACC_Area Command Teams\n",
      "GACC_NIMO Teams\n",
      "GACCAbbrev\n",
      "GACC_PL\n",
      "GACC_New fire\n",
      "GACC_Type 1 IMTs\n",
      "GACC_Uncont LF\n"
     ]
    }
   ],
   "source": [
    "# retrieve column names that have the number of missing values greater than 1 million\n",
    "cutoff_missing_values = 1000000\n",
    "cols_missing_gt1mil = list(missing_vals[missing_vals > cutoff_missing_values].index)\n",
    "print(f\"Number of columns with missing values greater than 1 million: {len(cols_missing_gt1mil)}\")\n",
    "print(\"Those columns are:\")\n",
    "print(*cols_missing_gt1mil, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46adeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns with missing values greater than 80%: 22\n",
      "Those columns are:\n",
      "IAHSEF\n",
      "IAULHS_89\n",
      "IAPLHS_88\n",
      "IALMIL_87\n",
      "ICS_209_PLUS_COMPLEX_JOIN_ID\n",
      "road_interstate_dis\n",
      "road_US_dis\n",
      "COMPLEX_NAME\n",
      "road_other_dis\n",
      "MTBS_ID\n",
      "MTBS_FIRE_NAME\n",
      "ICS_209_PLUS_INCIDENT_JOIN_ID\n",
      "road_state_dis\n",
      "road_county_dis\n",
      "NWCG_CAUSE_AGE_CATEGORY\n",
      "No_FireStation_1.0km\n",
      "Evacuation\n",
      "CheatGrass\n",
      "ExoticAnnualGrass\n",
      "Medusahead\n",
      "PoaSecunda\n",
      "FIRE_CODE\n"
     ]
    }
   ],
   "source": [
    "# retrieve column names that have the number of missing values greater than 80%\n",
    "cutoff_missing_values = df_init.shape[0]*0.8\n",
    "cols_missing_gt80pct = list(missing_vals[missing_vals > cutoff_missing_values].index)\n",
    "print(f\"Number of columns with missing values greater than 80%: {len(cols_missing_gt80pct)}\")\n",
    "print(\"Those columns are:\")\n",
    "print(*cols_missing_gt80pct, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f22deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that have missing values greater than 80%\n",
    "df_interim = df_init.drop(columns=cols_missing_gt80pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66431db",
   "metadata": {},
   "source": [
    "### Interim dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4720d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the interim dataset: 286\n",
      "The number of columns in the interim dataset: 2302521\n"
     ]
    }
   ],
   "source": [
    "# interim dataset\n",
    "# 1. how many columns does the interim dataset contain?\n",
    "print(f\"The number of columns in the interim dataset: {df_interim.shape[1]}\")\n",
    "\n",
    "# 2. how many rows are there in the interim dataset?\n",
    "print(f\"The number of columns in the interim dataset: {df_interim.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6840218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOCAL_FIRE_REPORT_ID       1824846\n",
       "geometry                   1730504\n",
       "GACC_Fire Use Teams        1710985\n",
       "GACCAbbrev                 1183441\n",
       "GACC_PL                    1183441\n",
       "GACC_New fire              1183441\n",
       "GACC_New LF                1183441\n",
       "GACC_Uncont LF             1183441\n",
       "GACC_Type 1 IMTs           1183441\n",
       "GACC_Type 2 IMTs           1183441\n",
       "GACC_NIMO Teams            1183441\n",
       "GACC_Area Command Teams    1183441\n",
       "FIRE_NAME                   995395\n",
       "CONT_TIME                   989572\n",
       "CONT_DATE                   894622\n",
       "CONT_DOY                    894622\n",
       "road_common_name_dis        817488\n",
       "No_FireStation_5.0km        797216\n",
       "DISCOVERY_TIME              787973\n",
       "LOCAL_INCIDENT_ID           744410\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. how many missing values are there in each column?\n",
    "df_interim.isna().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26a56de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2302521, 36)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how would the dataset looks like if columns with any missing data are dropped?\n",
    "df_interim.dropna(axis=1, how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe4f1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FOD_ID',\n",
       " 'FPA_ID',\n",
       " 'SOURCE_SYSTEM_TYPE',\n",
       " 'SOURCE_SYSTEM',\n",
       " 'NWCG_REPORTING_AGENCY',\n",
       " 'NWCG_REPORTING_UNIT_ID',\n",
       " 'NWCG_REPORTING_UNIT_NAME',\n",
       " 'SOURCE_REPORTING_UNIT',\n",
       " 'SOURCE_REPORTING_UNIT_NAME',\n",
       " 'FIRE_YEAR',\n",
       " 'DISCOVERY_DATE',\n",
       " 'DISCOVERY_DOY',\n",
       " 'NWCG_CAUSE_CLASSIFICATION',\n",
       " 'NWCG_GENERAL_CAUSE',\n",
       " 'FIRE_SIZE',\n",
       " 'FIRE_SIZE_CLASS',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'OWNER_DESCR',\n",
       " 'STATE',\n",
       " 'NPL',\n",
       " 'rpms',\n",
       " 'rpms_1km',\n",
       " 'FRG',\n",
       " 'Aspect_1km',\n",
       " 'Elevation_1km',\n",
       " 'Elevation',\n",
       " 'Slope_1km',\n",
       " 'Aspect',\n",
       " 'Slope',\n",
       " 'GHM',\n",
       " 'Annual_etr',\n",
       " 'Annual_precipitation',\n",
       " 'NDVI_min',\n",
       " 'NDVI_max',\n",
       " 'NDVI_mean']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_interim.dropna(axis=1, how='any').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba21a5",
   "metadata": {},
   "source": [
    "Only 36 variables are left if we were to drop cols with any missing values. Many interesting variables were dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350ab75",
   "metadata": {},
   "source": [
    "### Get to know period covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb9b057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2007-01-01\n",
       "1          2007-01-01\n",
       "2          2007-01-01\n",
       "3          2007-01-01\n",
       "4          2007-01-01\n",
       "              ...    \n",
       "2302516    2003-12-31\n",
       "2302517    2003-12-31\n",
       "2302518    2003-12-31\n",
       "2302519    2003-12-31\n",
       "2302520    2003-12-31\n",
       "Name: DISCOVERY_DATE, Length: 2302521, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interim.DISCOVERY_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7541f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interim['DISCOVERY_DATE'] = pd.to_datetime(df_interim.DISCOVERY_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e284d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period covered: 1992-01-01 00:00:00 to 2020-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Time period covered:',\n",
    "    df_interim.DISCOVERY_DATE.min(), 'to',\n",
    "    df_interim.DISCOVERY_DATE.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d83e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DISCOVERY_DATE\n",
       "1992     67961\n",
       "1993     61972\n",
       "1994     75935\n",
       "1995     71440\n",
       "1996     75561\n",
       "1997     61442\n",
       "1998     68356\n",
       "1999     89350\n",
       "2000     96397\n",
       "2001     87001\n",
       "2002     76128\n",
       "2003     68279\n",
       "2004     69371\n",
       "2005     92920\n",
       "2006    117944\n",
       "2007     98832\n",
       "2008     88322\n",
       "2009     81477\n",
       "2010     85578\n",
       "2011     98904\n",
       "2012     74258\n",
       "2013     66434\n",
       "2014     70480\n",
       "2015     77274\n",
       "2016     81994\n",
       "2017     82279\n",
       "2018     80863\n",
       "2019     62548\n",
       "2020     73221\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yr_counts = df_interim.DISCOVERY_DATE.dt.year.value_counts().sort_index()\n",
    "yr_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfa0d2",
   "metadata": {},
   "source": [
    "Because of the systemic change in our climate, e.g., climate crisis, there is likely time series element in play. Therefore, splitting randomly may not be the best option.\n",
    "One reference that we could use is the example of predicting COVID-19 growth rate in week 03. In the COVID-19 growth example, the data was split based on timeline - in which training data is the first 20 day, and test data is the last 10 day. \n",
    "\n",
    "If we were to use simliar approach, we would choose a cutoff year. That is, main training dataset (to include training and validation) will be examples (i.e., fire incidents) from year 1992 up to the cutoff year. And the test dataset will be examples in years after the cutoff year.\n",
    "\n",
    "Below is code to find out the cutoff year, should we reserve 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d3af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total observations: 2302521\n",
      "20% of total obs: 460504\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total observations: {yr_counts.sum()}\") \n",
    "print(f\"20% of total obs: {round(yr_counts.sum()*0.2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08e6c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total observations from year 2014 to 2020: 528659\n",
      "It is larger than or equal to 20% of total obs: True\n"
     ]
    }
   ],
   "source": [
    "cutoff_yr = 2014            # try 2014 (since 2015 and later years do not amount to 20% of total observations)\n",
    "print(f\"Total observations from year {cutoff_yr} to 2020: {yr_counts[yr_counts.index >= cutoff_yr].sum()}\")\n",
    "print(f\"It is larger than or equal to 20% of total obs: {yr_counts[yr_counts.index >= cutoff_yr].sum() >= round(yr_counts.sum()*0.2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
